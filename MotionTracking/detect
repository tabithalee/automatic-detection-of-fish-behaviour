import cv2
import numpy as np
import time
import matplotlib.pyplot as plt
from collections import deque


cap = cv2.VideoCapture('/home/tabitha/Desktop/automatic-detection-of-fish-behaviour/good_vids/'
                       'BC_POD1_PTILTVIDEO_20110519T091755.000Z_1.ogg')

myList = []

prevFrame = None
minArea = 750
threshVal = 10
dilationIterations = 3
gaussianSize = (61, 61)
bufferLength = 72
lineThick = 2

trackedPoints = deque(maxlen=bufferLength)
predictedPoints = deque(maxlen=bufferLength)

saliency = cv2.saliency.StaticSaliencyFineGrained_create()

# Create the Kalman Filter
kalmanX = cv2.KalmanFilter(1, 1, controlParams=0)
kalmanY = cv2.KalmanFilter(1, 1, controlParams=0)

# TODO - i don't understand this yet!!!
'''
kalmanX.transitionMatrix = np.array([[1., 0.], [0., 1.]])
kalmanX.measurementMatrix = 1. * np.ones((1, 2))
kalmanX.processNoiseCov = 1e-5 * np.eye(2)  # can play with this later... how much disturbance there is?
kalmanX.measurementNoiseCov = 1e-3 * np.ones((1, 1))
kalmanX.errorCovPost = 1. * np.ones((2, 2))
kalmanX.statePost = 0.1 * np.random.randn(2, 1)  # i guess this is adding some noise...?

kalmanY.transitionMatrix = np.array([[1., 0.], [0., 1.]])
kalmanY.measurementMatrix = 1. * np.ones((1, 2))
kalmanY.processNoiseCov = 1e-5 * np.eye(2)
kalmanY.measurementNoiseCov = 1e-3 * np.ones((1, 1))
kalmanY.errorCovPost = 1. * np.ones((2, 2))
kalmanY.statePost = 0.1 * np.random.randn(2, 1)
'''

# Check if camera opened successfully
if cap.isOpened() is False:
    print("Error opening video stream or file")

# Create a new figure for plt
fig, figarray = plt.subplots(2, 1)
# Read until video is completed
while cap.isOpened():
    # Capture frame-by-frame
    ret, frame = cap.read()
    if ret:
        initFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        frame = cv2.GaussianBlur(initFrame, gaussianSize, 0)
        frame = cv2.bilateralFilter(frame, 15, 80, 80, 0)

        if prevFrame is None:
            prevFrame = frame

        frameDiff = cv2.absdiff(prevFrame, frame)
        # _, frameDiff = saliency.computeSaliency(frameDiff)
        # frameDiff = (frameDiff * 255).astype('uint8')
        _, threshDiff = cv2.threshold(frameDiff, threshVal, 255, cv2.ADAPTIVE_THRESH_MEAN_C)

        threshDiff = cv2.dilate(threshDiff, None, iterations=dilationIterations)
        contours, _ = cv2.findContours(threshDiff.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for c in contours:
            if cv2.contourArea(c) < minArea:
                continue

            # Kalman prediction state
            predictionX = kalmanX.predict()
            predictionY = kalmanY.predict()

            predictedPoints.append((predictionX, predictionY))

            # find center of the contour
            myMoment = cv2.moments(c)
            xMom = int(myMoment['m10'] / myMoment['m00'])
            yMom = int(myMoment['m01'] / myMoment['m00'])

            # Check if the centroids are very different from the last one
            trackedPoints.appendleft((xMom, yMom))


            # Kalman update step
            testX = np.array(xMom, dtype=np.float32).reshape((1,))
            testY = np.array(yMom, dtype=np.float32).reshape((1,))
            kalmanX.correct(testX)
            kalmanY.correct(testY)

            print('prediction: ', predictionX, 'measurement: ', testX)

            # Draw bounding box(es)
            (xPos, yPos, width, height) = cv2.boundingRect(c)

            cv2.rectangle(initFrame, (xPos, yPos), (xPos + width, yPos + height), (0, 255, 0), 2)

            # loop through the tracked points
            for i in range(1, len(trackedPoints)):
                cv2.line(initFrame, trackedPoints[i-1], trackedPoints[i], (0, 0, 255), lineThick)
                cv2.line(initFrame, predictedPoints[i-1], predictedPoints[i], (0, 255, 0), lineThick)

        figarray[0].imshow(threshDiff)
        figarray[1].imshow(initFrame)
        plt.pause(0.001)

        myList.append(threshDiff)
        time.sleep(0.001)

    # Break the loop
    else:
        break

# When everything done, release the video capture object
cap.release()

# Closes all the frames
cv2.destroyAllWindows()

# convert list to array
# myArray = np.array(myList) may not need this for now

print('all frames saved')

'''
The following function tries to see if there are any fish in the frame with background subtraction. Usually background
subtraction is done with the first frame and the current frame but will see if it works the same with the previous frame
and current frame. 
'''

